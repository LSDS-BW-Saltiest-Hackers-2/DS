{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "negative_author_rank.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZQ8zEOkJ8rL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uPFjq3aR_23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v5XWwW0TStA",
        "colab_type": "code",
        "outputId": "e631553d-03ca-480d-f872-4d1e9a523a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t9XcEvGmA1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "libhAVCzT64z",
        "colab_type": "code",
        "outputId": "115bd384-7770-47d2-de16-2bbea5d2e671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR9st-sdVSUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyze_sentiment_vader_lexicon(review, threshold=0.4,\n",
        "                                    verbose=False):\n",
        "  # analyze the sentiment for review\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  scores = analyzer.polarity_scores(review)\n",
        "  # get aggregate scores and final sentiment\n",
        "  agg_score = scores['compound']\n",
        "  if agg_score >= threshold:\n",
        "    final_sentiment = round(scores['compound'], 2)*100\n",
        "  elif agg_score <= -threshold:\n",
        "    final_sentiment = round(scores['compound'], 2)*100\n",
        "  else:\n",
        "    final_sentiment = round(scores['neu'], 2)*100\n",
        "  if verbose:\n",
        "    # display detailed sentiment statistics\n",
        "    positive = str(round(scores['pos'], 2)*100)+'%'\n",
        "    final = round(agg_score, 2)\n",
        "    negative = str(round(scores['neg'], 2)*100)+'%'\n",
        "    neutral = str(round(scores['neu'], 2)*100)+'%'\n",
        "    sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n",
        "                                      negative, neutral]], columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'],['Predicted Sentiment', 'Polarity Score','Positive', 'Negative', 'Neutral']],codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
        "    print(sentiment_frame)\n",
        "  return scores['neg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCDKhTWYVtBP",
        "colab_type": "code",
        "outputId": "077659d2-d62a-4efb-c530-8040b60fa74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "analyze_sentiment_vader_lexicon('bad comment')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPrTku67VwrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Saltiest-Hackers/ML-Engineering/master/data/raw/hn_0.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVcxpP-bAmX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alSLSVFjXpDg",
        "colab_type": "code",
        "outputId": "b3f324c8-dd33-4bd2-8a32-9c27ac0082d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "analyze_sentiment_vader_lexicon(df['text'][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlOnS5VIgQNg",
        "colab_type": "code",
        "outputId": "9a41f226-dbfd-4598-fec5-5b8c5848f59a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "df = df.dropna(subset=['text'])\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>by</th>\n",
              "      <th>author</th>\n",
              "      <th>time</th>\n",
              "      <th>time_ts</th>\n",
              "      <th>text</th>\n",
              "      <th>parent</th>\n",
              "      <th>deleted</th>\n",
              "      <th>dead</th>\n",
              "      <th>ranking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10379195</td>\n",
              "      <td>rhaps0dy</td>\n",
              "      <td>rhaps0dy</td>\n",
              "      <td>1444725842</td>\n",
              "      <td>2015-10-13 08:44:02+00:00</td>\n",
              "      <td>I&amp;#x27;m not sure how much of it has been prov...</td>\n",
              "      <td>10379167</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10379193</td>\n",
              "      <td>pixelHD</td>\n",
              "      <td>pixelHD</td>\n",
              "      <td>1444725804</td>\n",
              "      <td>2015-10-13 08:43:24+00:00</td>\n",
              "      <td>Minecraft eh? I did the same with GTA San Andr...</td>\n",
              "      <td>10377203</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10379192</td>\n",
              "      <td>test1235</td>\n",
              "      <td>test1235</td>\n",
              "      <td>1444725705</td>\n",
              "      <td>2015-10-13 08:41:45+00:00</td>\n",
              "      <td>I think the closest universal interest for me ...</td>\n",
              "      <td>10372063</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10379190</td>\n",
              "      <td>copsarebastards</td>\n",
              "      <td>copsarebastards</td>\n",
              "      <td>1444725645</td>\n",
              "      <td>2015-10-13 08:40:45+00:00</td>\n",
              "      <td>&amp;gt; Why do you care about the success of the ...</td>\n",
              "      <td>10376640</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>10379189</td>\n",
              "      <td>spike021</td>\n",
              "      <td>spike021</td>\n",
              "      <td>1444725623</td>\n",
              "      <td>2015-10-13 08:40:23+00:00</td>\n",
              "      <td>What kinds of ramifications would there be if ...</td>\n",
              "      <td>10378759</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0        id               by  ... deleted  dead ranking\n",
              "0           0  10379195         rhaps0dy  ...     NaN   NaN       0\n",
              "1           1  10379193          pixelHD  ...     NaN   NaN       0\n",
              "2           2  10379192         test1235  ...     NaN   NaN       0\n",
              "3           3  10379190  copsarebastards  ...     NaN   NaN       0\n",
              "4           4  10379189         spike021  ...     NaN   NaN       1\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99vYYnr0doAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text'] = df['text'].astype(str)\n",
        "subset = df.sample(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0O1A7Y0ajLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subset['sent_score'] = subset['text'].apply(analyze_sentiment_vader_lexicon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9jhOWniamMD",
        "colab_type": "code",
        "outputId": "bcb3aae9-8cb7-44f6-de0d-324ea87ca8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "subset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>by</th>\n",
              "      <th>author</th>\n",
              "      <th>time</th>\n",
              "      <th>time_ts</th>\n",
              "      <th>text</th>\n",
              "      <th>parent</th>\n",
              "      <th>deleted</th>\n",
              "      <th>dead</th>\n",
              "      <th>ranking</th>\n",
              "      <th>sent_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97122</th>\n",
              "      <td>97122</td>\n",
              "      <td>10261144</td>\n",
              "      <td>cdvonstinkpot</td>\n",
              "      <td>cdvonstinkpot</td>\n",
              "      <td>1442951176</td>\n",
              "      <td>2015-09-22 19:46:16+00:00</td>\n",
              "      <td>The big companies might be on one of these occ...</td>\n",
              "      <td>10261043</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115095</th>\n",
              "      <td>115095</td>\n",
              "      <td>10239230</td>\n",
              "      <td>cryoshon</td>\n",
              "      <td>cryoshon</td>\n",
              "      <td>1442584964</td>\n",
              "      <td>2015-09-18 14:02:44+00:00</td>\n",
              "      <td>Of course the data was held back, it would hur...</td>\n",
              "      <td>10238112</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8866</th>\n",
              "      <td>8866</td>\n",
              "      <td>10368501</td>\n",
              "      <td>dpifke</td>\n",
              "      <td>dpifke</td>\n",
              "      <td>1444550751</td>\n",
              "      <td>2015-10-11 08:05:51+00:00</td>\n",
              "      <td>There&amp;#x27;s a DNSBL for that:&lt;p&gt;&lt;a href=\"http...</td>\n",
              "      <td>10367653</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60053</th>\n",
              "      <td>60053</td>\n",
              "      <td>10306460</td>\n",
              "      <td>Fiahil</td>\n",
              "      <td>Fiahil</td>\n",
              "      <td>1443641397</td>\n",
              "      <td>2015-09-30 19:29:57+00:00</td>\n",
              "      <td>Usually, festivals require that the movie is n...</td>\n",
              "      <td>10306360</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61802</th>\n",
              "      <td>61802</td>\n",
              "      <td>10304292</td>\n",
              "      <td>bradleyjg</td>\n",
              "      <td>bradleyjg</td>\n",
              "      <td>1443624127</td>\n",
              "      <td>2015-09-30 14:42:07+00:00</td>\n",
              "      <td>Eight restaurants reviewed in Queens in twenty...</td>\n",
              "      <td>10303402</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0        id             by  ... dead  ranking sent_score\n",
              "97122        97122  10261144  cdvonstinkpot  ...  NaN        2       0.00\n",
              "115095      115095  10239230       cryoshon  ...  NaN        4       0.13\n",
              "8866          8866  10368501         dpifke  ...  NaN        0       0.00\n",
              "60053        60053  10306460         Fiahil  ...  NaN        0       0.00\n",
              "61802        61802  10304292      bradleyjg  ...  NaN        1       0.00\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI2Q7ThUAvpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "authors_rank = (subset.groupby('author')['sent_score'].sum())/(subset.groupby('author')['text'].count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOAkcOY9EnpS",
        "colab_type": "code",
        "outputId": "0aa00c33-399c-469d-d6f8-431c72a606c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "len(subset['author'].unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBYDj5KJaHwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toint(content):\n",
        "  \n",
        "  return int(round(content,2)*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI0udMiaeWVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = subset['text'].values\n",
        "y_train = subset['sent_score'].apply(toint).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd0lh54GZdiu",
        "colab_type": "code",
        "outputId": "ebfed32a-cb1a-424c-f00a-9392f32d1922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 13,  0,  0,  0,  0,  0,  0,  2,  5,  0,  0,  0,  0,  0,  0,  2,\n",
              "       11, 14, 10, 18,  9,  0,  0, 13, 35,  4,  7,  3,  0,  0,  2,  0,  9,\n",
              "        0, 15,  6,  6,  5,  0,  0,  0,  0, 15, 17,  2,  0, 12,  1,  4,  0,\n",
              "       20,  0,  0,  9,  0,  0,  0,  0,  0,  4, 22,  2,  0,  0,  0,  7,  7,\n",
              "        0,  0,  0, 10,  0,  0,  6,  0,  9, 11, 12,  0,  0,  0,  6, 13, 26,\n",
              "        0, 19, 16, 11,  0,  0, 12,  6,  0,  8,  0, 19,  0,  0,  0, 19,  0,\n",
              "       14,  4,  2,  0, 16, 13,  0,  0,  4,  0,  8,  0, 42,  0,  3, 12, 11,\n",
              "        9,  2, 15, 12,  6,  0,  0, 21,  6,  0,  0, 21,  0,  7,  0, 10,  6,\n",
              "       28,  0, 15,  0,  0,  6,  0,  7,  0,  0, 22,  0, 23,  7,  1,  9, 25,\n",
              "        5,  0,  8,  0,  6,  0,  0, 10,  0,  3,  0, 19,  0,  0,  7, 12, 24,\n",
              "        0,  7,  0,  0,  0,  1, 12,  8,  0,  7,  0,  5,  2,  6, 15,  0,  5,\n",
              "        0,  6,  0,  6, 13,  7,  0,  0, 13,  0,  0,  3,  9,  4,  8,  5, 31,\n",
              "        5,  0,  0, 12, 16,  0,  0,  0,  0,  0,  3,  0,  3, 10,  7,  0,  6,\n",
              "        8, 14,  0,  7,  0,  3,  0, 27,  9,  0,  0,  0,  0,  8,  0, 12,  2,\n",
              "        0, 10,  6,  0,  0,  0,  2,  4,  0,  7, 19, 18,  0, 16, 16,  0,  0,\n",
              "       13, 11,  6,  0,  3,  7,  0,  0,  8,  6,  0,  6,  0, 15,  0,  8, 14,\n",
              "        8,  8,  4,  0,  0, 10, 38,  9, 11,  0,  0,  0, 15, 47,  0, 20,  3,\n",
              "        0,  0,  0, 39,  3,  9,  0, 14,  2,  0, 20, 11, 13, 11,  6,  0,  0,\n",
              "       10,  0,  0,  0,  0,  4,  8,  0,  0,  5,  4,  5, 15,  5,  0,  0,  6,\n",
              "        6,  0,  7,  4,  6, 22,  0,  6,  0,  0, 32,  0,  3,  8,  2,  0,  0,\n",
              "        0, 10,  0,  0, 12, 15, 13,  3,  0, 34, 15,  0, 26,  0,  6,  9,  3,\n",
              "        4,  0,  6,  9,  0, 10,  9,  0,  7, 33,  5, 12,  0,  0,  0,  5,  0,\n",
              "        0, 20,  3,  0, 17,  0,  9,  0,  6, 14,  4,  6,  0,  0,  0, 18,  0,\n",
              "        0,  5,  0,  0, 12,  5,  6, 12,  0,  8,  5,  0,  6,  3,  6, 26, 12,\n",
              "        0, 31,  0,  0,  0,  2,  0,  0, 30,  0, 10,  0,  9,  0,  0,  0,  0,\n",
              "        0, 18,  0, 28, 17,  6,  8,  0,  0, 13,  4,  0, 10, 15,  0, 12,  0,\n",
              "       16,  0,  0,  9,  0, 16,  3,  7,  0,  0,  0, 11,  0,  0,  0,  3,  1,\n",
              "        0,  0,  0,  4,  0, 10,  0,  0, 20,  0,  7,  0, 19,  3, 27, 28,  0,\n",
              "       13, 10,  0,  0,  8, 14,  6,  0,  6, 13,  0,  0, 14, 13,  5,  0,  0,\n",
              "        6,  0,  6,  0,  0,  4,  0,  0,  0,  3,  6,  6,  4,  5,  0,  7,  0,\n",
              "        6,  0, 19,  7,  4,  8,  5, 28, 12,  2, 10, 14,  5, 13, 14, 12,  0,\n",
              "        3,  0,  0,  0,  5,  0, 24,  0,  0,  4,  0, 13,  2, 14,  0,  0,  2,\n",
              "        9,  0,  8, 11,  8,  7,  0,  6, 12,  2,  0,  0,  0,  9,  0, 11,  0,\n",
              "        0,  0,  0,  0,  4, 21, 24,  0,  0,  4,  2,  0,  5, 10, 10,  4,  6,\n",
              "        0,  0,  2,  4,  0,  0,  0,  5, 16,  0,  0,  4, 14,  4, 16, 11,  0,\n",
              "        8,  0, 11,  0,  7,  5,  2,  6,  0,  3,  1,  0,  3,  7, 22, 44,  0,\n",
              "       12,  4,  0,  9,  6,  2,  0, 26,  0,  0,  4,  0,  3,  8,  0,  9,  1,\n",
              "        0,  0,  9,  0,  8,  8,  5,  0, 39,  0,  8,  5,  0,  4,  8, 11,  0,\n",
              "        0,  0,  0,  0, 12,  0,  0,  1,  0, 52, 11,  3,  0, 26,  7,  5,  5,\n",
              "        0,  0,  5,  0,  1,  4,  4,  3,  4,  7,  9,  0,  0, 15,  2,  0,  4,\n",
              "        0,  0,  0,  9,  2, 23, 16,  8, 11,  2,  6,  0, 10,  0, 19,  2,  0,\n",
              "        3,  5,  0,  0,  0,  9,  0,  3,  8,  0, 11, 16,  0,  0,  4, 13,  0,\n",
              "       18,  0,  0, 22,  9,  0,  0, 11,  0,  0,  0,  3, 11,  0, 14,  0,  2,\n",
              "        2, 23, 10,  0,  0,  3, 13, 13, 10,  0,  0,  0,  3,  9,  8, 18, 18,\n",
              "        0, 15,  0,  3,  0,  0,  9,  4,  2, 18,  7, 12,  0, 14,  4,  0,  7,\n",
              "        6,  0,  5,  6,  4, 10,  0,  0,  2,  9,  0,  8,  0,  0, 11, 17,  9,\n",
              "        0, 10,  0, 19,  0,  0, 20,  4,  0,  0, 10, 17,  0,  0,  6,  0,  4,\n",
              "        0, 14, 11,  7,  8,  0, 15, 13, 20,  0,  7, 17,  0,  5,  0, 20,  0,\n",
              "        8,  0,  9,  4,  2,  3,  0,  0,  7, 20,  0,  0,  1,  8,  0,  1,  0,\n",
              "        0, 10,  4,  0,  0,  8, 16,  0,  5,  0,  0,  0, 14, 12,  0, 14, 27,\n",
              "        0, 10,  0,  0,  0, 12,  0,  8, 10,  3,  0,  0, 12,  0, 14,  0,  0,\n",
              "       12,  0,  0,  0,  8,  2,  0,  0, 14, 19, 13,  0,  0,  6, 12,  0,  0,\n",
              "        3, 23,  0,  7,  8,  7, 23,  8,  0, 16,  0, 12,  0,  0,  0, 20, 10,\n",
              "        7, 16, 41,  9,  8,  3,  8,  5,  7,  0,  0,  0,  0, 10,  0,  9,  6,\n",
              "       15, 10,  0, 14,  7,  0,  2, 18,  0, 22,  0,  0,  0,  6,  6,  0,  3,\n",
              "       20,  0, 20,  0,  0,  6,  0, 21,  0,  0,  0,  8,  5,  9, 17, 17,  0,\n",
              "        0, 12,  3,  5,  8,  0,  0, 10, 10,  1,  6,  0,  5, 28,  7,  3, 24,\n",
              "        6,  0,  0,  2,  3,  0, 11, 12,  9, 28, 13,  0, 10,  0,  0,  5,  2,\n",
              "        0, 25,  0,  6,  0,  0, 14,  0,  0,  7,  0,  0,  7,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPgSKEX9k_Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = []\n",
        "for doc in tokenizer.pipe(x_train, batch_size=500):\n",
        "  doc_tokens = []\n",
        "  for token in doc:\n",
        "      doc_tokens.append(token.text.lower())\n",
        "  tokens.append(doc_tokens)\n",
        "x_train = tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6T9mkeYlLLz",
        "colab_type": "code",
        "outputId": "3b00ffdb-26d2-4912-e62c-6eea57d9e57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "x_train = [\" \".join(row) for row in x_train]\n",
        "# X_test = [\" \".join(row) for row in X_test]\n",
        "\n",
        "x_train = vect.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_train.todense(), columns=vect.get_feature_names())\n",
        "x_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>00 1983</th>\n",
              "      <th>00 using</th>\n",
              "      <th>000</th>\n",
              "      <th>000 00</th>\n",
              "      <th>000 000</th>\n",
              "      <th>000 fully</th>\n",
              "      <th>000 guess</th>\n",
              "      <th>000 lbs</th>\n",
              "      <th>000 noticeable</th>\n",
              "      <th>000 objects</th>\n",
              "      <th>000 place</th>\n",
              "      <th>000 probably</th>\n",
              "      <th>000 released</th>\n",
              "      <th>000 times</th>\n",
              "      <th>000 vacant</th>\n",
              "      <th>000 year</th>\n",
              "      <th>003</th>\n",
              "      <th>003 code</th>\n",
              "      <th>00s</th>\n",
              "      <th>00s images</th>\n",
              "      <th>01</th>\n",
              "      <th>01 x2f</th>\n",
              "      <th>01191</th>\n",
              "      <th>01191 pdf</th>\n",
              "      <th>015</th>\n",
              "      <th>015 1e5</th>\n",
              "      <th>015 city</th>\n",
              "      <th>02</th>\n",
              "      <th>02 x2f</th>\n",
              "      <th>02551</th>\n",
              "      <th>02551 rel</th>\n",
              "      <th>02551 relevant</th>\n",
              "      <th>03</th>\n",
              "      <th>03 x2f</th>\n",
              "      <th>04</th>\n",
              "      <th>04 sure</th>\n",
              "      <th>04 x2f</th>\n",
              "      <th>05</th>\n",
              "      <th>05 x2f</th>\n",
              "      <th>...</th>\n",
              "      <th>zealand</th>\n",
              "      <th>zealand large</th>\n",
              "      <th>zealand time</th>\n",
              "      <th>zephyr</th>\n",
              "      <th>zephyr directions</th>\n",
              "      <th>zero</th>\n",
              "      <th>zero add</th>\n",
              "      <th>zero caffeine</th>\n",
              "      <th>zero desire</th>\n",
              "      <th>zero nrz</th>\n",
              "      <th>zero phone</th>\n",
              "      <th>zero result</th>\n",
              "      <th>zero school</th>\n",
              "      <th>zero transferred</th>\n",
              "      <th>zeromq</th>\n",
              "      <th>zeromq mongodb</th>\n",
              "      <th>zeros</th>\n",
              "      <th>zeros x27</th>\n",
              "      <th>zfs</th>\n",
              "      <th>zfs perform</th>\n",
              "      <th>zillow</th>\n",
              "      <th>zillow fall</th>\n",
              "      <th>zillow hand</th>\n",
              "      <th>zip</th>\n",
              "      <th>zip file</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombie cookie</th>\n",
              "      <th>zones</th>\n",
              "      <th>zones supposed</th>\n",
              "      <th>zoning</th>\n",
              "      <th>zoning ordinance</th>\n",
              "      <th>zoomermag</th>\n",
              "      <th>zoomermag com</th>\n",
              "      <th>zzzzzzzzzzzzzzzzzzzz</th>\n",
              "      <th>ʌmma</th>\n",
              "      <th>ʌmma 아빠</th>\n",
              "      <th>아빠</th>\n",
              "      <th>아빠 appa</th>\n",
              "      <th>엄마</th>\n",
              "      <th>엄마 eomma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 38198 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    00  00 1983  00 using  000  000 00  ...  ʌmma 아빠   아빠  아빠 appa   엄마  엄마 eomma\n",
              "0  0.0      0.0       0.0  0.0     0.0  ...      0.0  0.0      0.0  0.0       0.0\n",
              "1  0.0      0.0       0.0  0.0     0.0  ...      0.0  0.0      0.0  0.0       0.0\n",
              "2  0.0      0.0       0.0  0.0     0.0  ...      0.0  0.0      0.0  0.0       0.0\n",
              "3  0.0      0.0       0.0  0.0     0.0  ...      0.0  0.0      0.0  0.0       0.0\n",
              "4  0.0      0.0       0.0  0.0     0.0  ...      0.0  0.0      0.0  0.0       0.0\n",
              "\n",
              "[5 rows x 38198 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geTuIc3rmU5-",
        "colab_type": "code",
        "outputId": "46d5c93b-19a1-440e-c0ce-d17abd81198a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(penalty='l2', max_iter=100)\n",
        "\n",
        "lr.fit(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RftOXLpMmgcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = vect.transform(['really bad comment','I love this'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzJBAaA672LL",
        "colab_type": "code",
        "outputId": "1cf9df78-63de-41c1-e17b-2b9b28914325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "lr.predict(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0F5c3NZc2BU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = subset['text'].values\n",
        "y_train = subset['sent_score'].apply(toint).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muk-L3nj75EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # strings to token integer counts\n",
        "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
        "    ('classifier', LogisticRegression(random_state=101)),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
        "])\n",
        "\n",
        "pipeline.fit(x_train,y_train)\n",
        "predictions = pipeline.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWJgTy1xcPk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subset['pred'] = predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5SOADyzc54P",
        "colab_type": "code",
        "outputId": "57c520c7-4e99-4a4d-a44b-5ae511f7cae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "subset[subset['pred'] > 6]['text'].values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['As someone who is reasonably familiar with term sheets, I feel embarrassed that I never gave much thought to the moral hazard that liquidation-preference clauses create.<p>Viewed from the perspective of a founder, they are a serious annoyance. But from the perspective of the public at large, they are being exploited to engage in pseudo-fraud.<p>Right now, the world treats a company&#x27;s &#x27;valuation&#x27; as a reliable signal of information about how much investors actually believe the company is worth. That information is then integrated into heuristics that influence various people&#x27;s decisions: Will a paper write about a startup? Will a reader pay attention? Will a recruit take the company seriously? A billion dollar valuation goes a long way in each circumstance.<p>The problem is that our collective intuitions are using an outdated algorithm for assessing value. In theory, VCs who invest at a given valuation are providing reliable information by putting their money where their mouth is. But in practice, liquidation preference means that the official valuations attached to their deals don&#x27;t really reflect the limited risks they are taking on.<p>A VC who invests $10 million at $1 billion valuation may officially be signaling to the world that a company is valuable. But if he insists on a two-times liquidation preference, then he is really indicating that when the whole thing blows up he wants to make sure he can get the first $20 million.<p>Unfortunately, that information isn&#x27;t broadcast the same way to outside decision makers. Even those of us who think that startups are grossly overvalued don&#x27;t give enough thought to how illusory those valuations are to the very investors whose capital is fueling them.<p>I hope that will start to change.',\n",
              "       'Not to trivialize his opinion, but I always thought it was a wide spread notion that not allowing anonymity will lead to only &quot;politically correct&quot; or &quot;harmless&quot; opinions to surface.<p>I made the same argument when one of the Norwegian newspapers I somewhat read regularly (online) started requiring full names in the comments sections. The comments sections are now completely boring and predictable, and any really important but sensitive issues that would really need a minority voice in the debate are just not debated at all.<p>At least not on a civil level. There are always those who blurt out anything even under their full name (although possibly fake), but these tend to be the same people who are not really able to articulate a sensible and constructive argument for their case.<p>Personally I have opinions I would not like to link to my real name, since I need to work for a living in a place where we have customers and such...',\n",
              "       'The car looks absolutely beautiful.  Sadly, as much as I want to go electric, I have no use for it, at $140K for an SUV I expect a lot more than this.<p>I think the falcon doors solve a non-existing problem while parking.  I can&#x27;t remember the last time this was an issue, with or without kids, with or without cargo or bags on the back seat of either an SUV or a minivan.<p>The same is true of the third row seat.  We&#x27;ve had both SUV&#x27;s and minivans with third rows.  Never an issue, even for adults.<p>The falcon wings are very cool from a technological perspective but a complete non-starter for me.  Bikes, surfboards and other stuff need to go on a roof rack.  I can&#x27;t see any way to transport our Maas rowing shells on this thing.  They are 24 feet long and have to go on the roof.  I&#x27;ve even brought home 2x4&#x27;s and sheets of plywood from Home Depot on top of SUV&#x27;s and minivans.<p>Can you open the falcon wing doors by hand if you have no power?  I&#x27;d be surprised if this was not possible.<p>Towing is another aspect of this car that is likely to disappoint outside of a stage.  If anyone expects to be able to do 250 miles while towing a 5,000 lbs trailer on anything other than an absolutely flat road with a 60 mile per hour tail winds they are going to be disappointed.  As drag increases so will current draw and electrical losses, which will be constant and very significant.  Weight is more of a factor on non-flat roads, which is nearly 100% of them anywhere I&#x27;ve been.  I&#x27;d be surprised if the actual range while towing is much more than half the rated non-towing range.<p>What&#x27;s good?<p>Crash performance is fantastic!<p>HEPA filter:  Unreal. Nice.<p>It looks amazing.<p>It&#x27;s electric!<p>I sincerely think Tesla should have come up with a new acronym for Model X:  LUV - Luxury Utility Vehicle.  That, no doubt, it is.',\n",
              "       'There were active resistances in most occupied countries including France.\\n<a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;French_Resistance\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;French_Resistance</a><p>However a big difference between occupied populations and Jews is that Germany for the most part left the occupied population alone.\\nYes they were still oppressed and in many cases fell into poverty however they weren&#x27;t hunted down by the SS, gathered and then shipped to concentration camps, ghettos, and later to extermination camps.<p>As far as Ghettos go then for the most part it was a &quot;Polish&quot; issue, there weren&#x27;t many Ghettos outside of Poland, while it appears now that the German population knew much more about the treatment of Jews than what previously thought or admitted the German&#x27;s kept Germany clean, and only had labor and work camps within Germany and for the most part France and other occupied countries which sadly provided much better conditions for the most part which enabled the Germans to dismiss the claims of what was happening in Poland and in the eastern front as Jewish and later Allied propaganda.<p>And this is a major issue, while French, Polish, Danish, Dutch and other nationalities had resistances they for the most part were fighting for ideals and their personal freedom. A population in risk of genocide has to literally fight for it&#x27;s life. If Jews the capabilities of supporting a continuous armed resistance the Holocaust could&#x27;ve been avoided (this isn&#x27;t claiming that the end result would&#x27;ve resulted in less deaths) because it would be much less likely that the Germans could&#x27;ve as easily transported as many Jews at that much of an ease during the final solution.<p>The final solution was so unthinkable that even the Jews didn&#x27;t believe it until the news broke out in the final year of the war in Europe, the majority of the Jews were transported from ghettos and work camps and thought that they are just are being put into a new camp, the level of deception that was put into the final solution was truly unbelievable including letting the &quot;red-cross&quot; (still no clear if this was a show or the actual red-cross) visit the Jews in the big Ghettos before transferring them to the death camps to further pacify and calm down the population.<p>So yes arming at risk populations might make a huge difference in the end even if you only consider that difference to be the way they died rather than the pure numbers, but in modern times it&#x27;s also an issue. Tribal genocide (which sadly being going since even before colonial times) cannot be just as easily compared to the Holocaust, if you are the population too much you might still get a Genocide in which the original at risk population isn&#x27;t the victim but now the perpetrator so you end up with constantly fueling an ever shifting conflict.',\n",
              "       'But we (Mozilla) are also doing many of the same things as the Chromium team here. In particular there is work in progress to automatically &quot;ignore&quot; the results of known-flaky tests until we detect that there has been a change in the <i>rate</i> of flakiness, at which point we will — assuming all goes to plan — trigger new test runs until we can determine the point at which the regression was introduced.<p>I think one of the lessons we&#x27;ve learnt is that with a browser-type project it&#x27;s very hard to make test runs fully deterministic, for both technical and human reasons.<p>The technical reasons are touched on in the original article: these are complex codebases with lots of moving parts and lots of environmental dependencies. Of course there are various tactics to try and combat this; for example there is a wiki page dedicated to innocuous-looking code that leads to intermittent tests [1].<p>The human reasons centre around the difficulty of getting people to care about spending time fixing a test that fails one time in 1,000 (which is still very noticeable when you are running it hundreds of times a day). Unless the issue is something that fits a known pattern it&#x27;s hard work, difficult to tell if your fix even worked, and not likely to be considered a top priority due to the diffuse, hard to quantify, nature of the benefits.<p>I think the fact that both Google and Mozilla still have significant problems with intermittents despite talented engineering staff and it having been a known problem for years implies that some of the standard thinking about making tests fully deterministic simply doesn&#x27;t apply; for this kind of work you have to embrace — or at least accept — the randomness, and look for ways to get the data you need despite the noise.<p>[1] <a href=\"https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Mozilla&#x2F;QA&#x2F;Avoiding_intermittent_oranges\" rel=\"nofollow\">https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Mozilla&#x2F;QA&#x2F;Avoiding...</a>',\n",
              "       'Lets just take this one line<p>&quot;Why is that? Because African Americans have an average IQ of 85 and only 11-16% have an IQ &gt; 100 (the national average).&quot;<p>As someone with an interest in this (both as an African and someone who measures freakishly high on IQ tests way beyond my actual abilities, joined MENSA on a lark), I’ve had this argument over and over again. Most of this goes back to the IQ statistics cited in “The Bell Curve” as applies to Africans. The “Bell Curve” was a Storefront tract dressed in respectable clothing (sadly, every 20 years it has a successor, the most recent is Nicholas Wade’s “A troublesome Inheritance&quot;. The statistic in question was quoted as an average IQ of 75 for Black Africans. If you read the citations, it comes from a study quoted by Richard Lynn (infamous racist, google ‘Mankind Quaterly’). It is based on a study done in South Africa in 1927 by the Boer Govt (it actually concluded that Bantu Africans has an IQ of 70 and was one of the intellectual foundations for Apartheid). I leave it as an exercise for people to figure out why that study might have been slightly biased.<p>Keep in mind that an IQ of 80 is borderline retarded. So to believe this you have to believe that almost every black person you run into is either borderline retarded or just above that.<p>Now, you’ve been down voted into the grey, which I think is good, but just in case people think you’re being down voted for being ‘politically incorrect’ rather than for being a racist, here are some recommended readings<p>Whistling Vivaldi: How Stereotypes Affect Us and What We Can Do - Claude M Steele<p>The Bell Curve Wars: Race, Intelligence, and the Future of America - Steven Fraser (Editor)<p>The Mismeasure of Man - Stephen Jay Gould<p>The “people are where they are because of some innate fault” is a very attractive argument that has been used time and time again (I know I used it growing up upper middle class in a country with a lot of poor people. If only those people were hard workers like me and pulled themselves by their bootstraps). It is used time and time again against African Americans, and it is ironic to see it argued here in an article that is one of 2 (with the linked ’The Case for Reparations’) that clearly lays out the case as to why African Americans are disadvantaged in the U.S.<p>There are None so Blind as those who refuse to see.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0TRfPx-dDDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uDJAHBSfHt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_FILEPATH = os.path.join(\"latest_model.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOwdCqq3gASH",
        "colab_type": "code",
        "outputId": "d9c04dbc-bbb0-400d-ba8e-da1342082828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "print(\"SAVING THE MODEL...\")\n",
        "with open(MODEL_FILEPATH, \"wb\") as model_file:\n",
        "  pickle.dump({\"model\": pipeline}, model_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAVING THE MODEL...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5trVWGUIgfVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model():\n",
        "    print(\"LOADING THE MODEL...\")\n",
        "    with open(MODEL_FILEPATH, \"rb\") as model_file:\n",
        "        saved_model = pickle.load(model_file)\n",
        "    return saved_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6fUWcrwhUXA",
        "colab_type": "code",
        "outputId": "6ca96700-5a95-43ac-8292-6c04ca7c527e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "package = load_model()\n",
        "model = package['model']\n",
        "pred = model.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOADING THE MODEL...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvmzTiN7heHy",
        "colab_type": "code",
        "outputId": "22bf8276-3604-4011-b005-d7c88a5f5ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        }
      },
      "source": [
        "pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  3,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,\n",
              "        0,  0,  0,  0,  0,  6,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  8,  0,  0,  6,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  6,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  4,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  9,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tmWYb8lh1AA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}