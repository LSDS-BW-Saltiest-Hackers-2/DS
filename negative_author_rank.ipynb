{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "negative_author_rank.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZQ8zEOkJ8rL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uPFjq3aR_23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ab20c07-6104-4eaa-8d02-e12c78be5d42"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v5XWwW0TStA",
        "colab_type": "code",
        "outputId": "fd8447cf-0ccc-4e45-82b6-6762f0ca91f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t9XcEvGmA1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "libhAVCzT64z",
        "colab_type": "code",
        "outputId": "478c16a2-5a04-4226-ba2d-76bde0b98585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR9st-sdVSUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyze_sentiment_vader_lexicon(review, threshold=0.4,\n",
        "                                    verbose=False):\n",
        "  # analyze the sentiment for review\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  scores = analyzer.polarity_scores(review)\n",
        "  # get aggregate scores and final sentiment\n",
        "  agg_score = scores['compound']\n",
        "  if agg_score >= threshold:\n",
        "    final_sentiment = round(scores['compound'], 2)*100\n",
        "  elif agg_score <= -threshold:\n",
        "    final_sentiment = round(scores['compound'], 2)*100\n",
        "  else:\n",
        "    final_sentiment = round(scores['neu'], 2)*100\n",
        "  if verbose:\n",
        "    # display detailed sentiment statistics\n",
        "    positive = str(round(scores['pos'], 2)*100)+'%'\n",
        "    final = round(agg_score, 2)\n",
        "    negative = str(round(scores['neg'], 2)*100)+'%'\n",
        "    neutral = str(round(scores['neu'], 2)*100)+'%'\n",
        "    sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n",
        "                                      negative, neutral]], columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'],['Predicted Sentiment', 'Polarity Score','Positive', 'Negative', 'Neutral']],codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
        "    print(sentiment_frame)\n",
        "  return scores['neg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCDKhTWYVtBP",
        "colab_type": "code",
        "outputId": "4943436f-4bac-4c49-ac13-d22070e39b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "analyze_sentiment_vader_lexicon('bad comment')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPrTku67VwrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df = pd.read_csv('https://raw.githubusercontent.com/Saltiest-Hackers/ML-Engineering/master/data/raw/hn_0.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjYJTRvmeqMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b80812f5-ad8e-4718-e23a-6c8e82cf37d9"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "largedatacomment.csv  latest_model.pkl  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVcxpP-bAmX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/largedatacomment.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alSLSVFjXpDg",
        "colab_type": "code",
        "outputId": "f0580b17-a4f6-4558-aafa-464fa32168b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "analyze_sentiment_vader_lexicon(df['Comment'][0])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlOnS5VIgQNg",
        "colab_type": "code",
        "outputId": "9a0e6202-0443-4848-e12e-1b98e13830d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df = df.dropna(subset=['Comment'])\n",
        "df.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment_ID</th>\n",
              "      <th>Comment</th>\n",
              "      <th>UserName</th>\n",
              "      <th>StoryId</th>\n",
              "      <th>Deleted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23341390</td>\n",
              "      <td>&amp;gt; Java 8 language support update: APIs you ...</td>\n",
              "      <td>pjmlp</td>\n",
              "      <td>23340887</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23342194</td>\n",
              "      <td>I’m still amazed and somewhat annoyed by the f...</td>\n",
              "      <td>krm01</td>\n",
              "      <td>23340887</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23341996</td>\n",
              "      <td>1. The selected Java SDK API desugaring, thoug...</td>\n",
              "      <td>pcx</td>\n",
              "      <td>23340887</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23341990</td>\n",
              "      <td>What program did they use to do the SpeakerVid...</td>\n",
              "      <td>TuringNYC</td>\n",
              "      <td>23340887</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23341822</td>\n",
              "      <td>&amp;gt; Clangd support for C++&lt;p&gt;That’s interesti...</td>\n",
              "      <td>dgellow</td>\n",
              "      <td>23340887</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Comment_ID  ... Deleted\n",
              "0    23341390  ...     NaN\n",
              "1    23342194  ...     NaN\n",
              "2    23341996  ...     NaN\n",
              "3    23341990  ...     NaN\n",
              "4    23341822  ...     NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99vYYnr0doAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.copy()\n",
        "df['Comment'] = df['Comment'].astype(str)\n",
        "subset = df.sample(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0O1A7Y0ajLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subset['sent_score'] = subset['Comment'].apply(analyze_sentiment_vader_lexicon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9jhOWniamMD",
        "colab_type": "code",
        "outputId": "7159986a-f9d3-4f28-9e07-bd5b6131b0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "subset.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment_ID</th>\n",
              "      <th>Comment</th>\n",
              "      <th>UserName</th>\n",
              "      <th>StoryId</th>\n",
              "      <th>Deleted</th>\n",
              "      <th>sent_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1971</th>\n",
              "      <td>23332590</td>\n",
              "      <td>Why?</td>\n",
              "      <td>edem</td>\n",
              "      <td>23321096</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>23334868</td>\n",
              "      <td>Seems interesting how he predicted how social ...</td>\n",
              "      <td>lanevorockz</td>\n",
              "      <td>23334463</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2648</th>\n",
              "      <td>23326631</td>\n",
              "      <td>Honestly, it&amp;#x27;s hard for me to take anythi...</td>\n",
              "      <td>dvdhnt</td>\n",
              "      <td>23322793</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>23339233</td>\n",
              "      <td>I actually ditched Docker on Windows with the ...</td>\n",
              "      <td>yndoendo</td>\n",
              "      <td>23337898</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2455</th>\n",
              "      <td>23323168</td>\n",
              "      <td>Crybaby cries, news at 11.</td>\n",
              "      <td>baggachipz</td>\n",
              "      <td>23322112</td>\n",
              "      <td>True</td>\n",
              "      <td>0.403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Comment_ID  ... sent_score\n",
              "1971    23332590  ...      0.000\n",
              "227     23334868  ...      0.000\n",
              "2648    23326631  ...      0.048\n",
              "403     23339233  ...      0.032\n",
              "2455    23323168  ...      0.403\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI2Q7ThUAvpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "authors_rank = (subset.groupby('UserName')['sent_score'].sum())/(subset.groupby('UserName')['Comment'].count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOAkcOY9EnpS",
        "colab_type": "code",
        "outputId": "8be53e6e-2fe5-4655-d597-5011639f2cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(subset['UserName'].unique())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "905"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBYDj5KJaHwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toint(content):\n",
        "  \n",
        "  return int(round(content,2)*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI0udMiaeWVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = subset['Comment'].values\n",
        "y_train = subset['sent_score'].apply(toint).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd0lh54GZdiu",
        "colab_type": "code",
        "outputId": "d6032c5e-9a9c-49f0-f9ac-dba2a2bffea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 14,  0, 15, 19, 17, 10, 12,  4,  4,  0,  0,  0,  0, 21,  4, 28,\n",
              "        9,  3,  0,  3, 19,  0,  6,  0,  0,  7,  9, 11,  0, 12,  0, 13, 42,\n",
              "        0, 12,  0,  3,  4,  7,  4,  7,  9,  5, 18,  4,  0,  0, 16, 10,  7,\n",
              "        8,  0,  6,  6, 20,  6, 43,  0,  0,  7,  7,  1,  0, 13,  0,  0, 10,\n",
              "        7, 22,  0,  2, 12,  0, 10,  0,  8,  4,  0, 12,  5,  5,  0,  4,  0,\n",
              "       12, 17, 17,  0,  9, 19,  5,  0, 13,  0, 22, 12, 10,  0, 18,  0,  0,\n",
              "        4,  0,  0,  6,  3,  0,  6,  8, 10,  0, 17, 12,  0, 27,  7,  0,  9,\n",
              "       15,  0, 11, 16,  0,  6,  0,  0, 12,  0,  3,  7,  0, 17,  9,  2,  6,\n",
              "        4,  0,  0,  2,  0,  3,  5,  3,  4,  0,  9, 13,  7, 12,  9,  8,  3,\n",
              "        5,  1, 14,  5,  3, 18,  0,  0,  0,  0,  4,  0, 10,  0, 26,  0,  0,\n",
              "       24, 13,  4, 13,  3,  7, 12,  0,  0,  0,  0,  0, 20,  0,  0,  0,  0,\n",
              "       35,  0,  0,  0,  0,  3, 18,  6,  0,  4,  0,  4,  6,  0, 10,  0,  0,\n",
              "        3,  0,  5,  0, 15, 21,  0,  0,  0,  0,  4, 28, 10,  0, 27,  0, 36,\n",
              "        0,  0, 17, 11, 13, 31,  0,  3,  0, 16,  0, 11,  0,  0,  0,  3,  5,\n",
              "       15,  0, 11,  0, 14, 15,  3,  0,  1, 11, 10,  8,  9,  0,  0,  0,  0,\n",
              "       15,  2,  0, 12, 10,  0,  0,  0,  0,  0, 13,  0, 23,  0,  0,  9,  0,\n",
              "        0,  4, 21,  0,  5,  5,  0,  8,  7,  0,  0,  0,  0,  3,  0,  0, 14,\n",
              "        3,  5,  0,  7,  0,  0,  0,  0,  0,  0, 12, 12,  0, 16,  4,  3,  0,\n",
              "        1,  3,  5,  0,  8, 32, 19,  0, 15, 17,  0,  0,  0, 18,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  8,  8,  0,  0,  9,  7,  2,  0, 15,  0,  0,\n",
              "        0, 10,  5,  0,  6,  9,  0,  5, 31,  0, 24,  4,  0, 11,  3,  6,  1,\n",
              "        0,  0, 20,  3,  3, 12,  0,  5,  5,  0, 10,  0, 69,  0,  6,  0, 11,\n",
              "        0,  0, 10,  0,  0,  0,  0,  0,  6,  0, 10,  0,  9,  0,  0,  0, 15,\n",
              "        5,  7,  2, 14, 11,  0, 12,  0, 19, 14,  0, 16,  0, 39,  3, 10, 11,\n",
              "        2,  0,  9,  0, 16,  0,  0, 13,  0, 16,  0,  0,  4,  4,  0, 19,  0,\n",
              "       18,  0,  3,  0, 16, 18,  0,  4, 13,  4,  0, 16,  7, 14,  0,  5,  0,\n",
              "       19,  0, 11, 15, 23,  5,  0,  0,  2,  4,  7, 17,  0,  7, 18,  0,  7,\n",
              "        7,  0,  0,  7,  0,  2,  0,  0, 27,  0,  5,  0, 19,  0,  3, 11,  0,\n",
              "        0,  7, 12, 12,  7, 31,  6,  6, 15,  0, 12, 26,  0, 25,  0,  0,  3,\n",
              "        8,  3,  0,  5,  5,  0,  0,  0, 13,  0,  0,  6, 11,  3,  0,  9, 19,\n",
              "        4,  0,  0, 78,  0, 11,  0, 12,  2,  0, 15, 10, 19,  4, 20,  0,  0,\n",
              "        0,  0, 21, 80,  3, 26,  0, 21,  8,  0,  0,  5,  0, 27,  4,  9,  0,\n",
              "        0, 18, 16,  6,  0, 11,  7,  9,  0, 18,  0, 20, 14,  0, 28,  0, 14,\n",
              "       22,  2,  8,  0,  7,  7,  9, 19, 10, 14,  4,  7,  7,  2,  0,  5,  3,\n",
              "        0, 21, 15, 15,  0,  0, 10,  2,  7, 28,  0,  0,  0, 22, 31,  7,  0,\n",
              "       15,  7,  0,  3,  2,  9,  4,  0, 22,  0,  9,  7, 22,  0,  5,  0,  7,\n",
              "       13, 19,  7,  4,  0,  0, 18,  9, 16,  7,  5,  0,  0,  0, 14,  9,  0,\n",
              "        0,  0,  6, 42,  6, 13,  0,  7,  0,  0,  2,  0,  2,  0,  6,  4, 20,\n",
              "       24,  0, 12,  8,  8,  0,  0,  0,  0, 30,  3, 11, 10,  3, 14, 20, 20,\n",
              "        0,  0, 12, 11,  0,  0,  8,  0,  0,  5,  0,  6, 20,  0,  7,  0, 11,\n",
              "        0,  5, 18,  4, 12,  9,  0,  2,  0, 28,  0,  0, 19,  0, 24, 22,  0,\n",
              "       34, 27,  7, 12,  0,  3,  0,  6, 14, 20,  0,  7, 17,  9, 10, 27,  1,\n",
              "        8,  8,  8,  3,  3, 16, 17,  0,  5,  0,  0, 14,  0,  0,  0,  7, 21,\n",
              "       14, 15,  0, 20,  5, 12, 10, 16,  4,  5,  0,  0,  0,  4,  0, 19,  7,\n",
              "        3,  0,  0,  0,  8,  7, 19,  0,  0,  0,  0,  4,  0, 18,  5,  6,  0,\n",
              "        6, 12, 19,  0, 11,  9,  5,  7,  0,  0,  6, 20,  0,  5, 10,  0, 17,\n",
              "        3, 22,  3,  0,  6, 26,  0, 21,  0,  0,  7,  7, 12,  0,  0, 35, 10,\n",
              "        5,  3,  0,  4,  0,  7, 11,  0,  0, 22,  0,  0,  1, 27,  6,  0,  0,\n",
              "        4, 14, 10,  5,  7,  0,  2,  0,  5,  3,  8,  0, 12,  0, 16,  0,  0,\n",
              "        0, 25, 11,  3,  0, 33,  3,  3,  6, 10,  0, 11, 19,  0, 11,  0, 11,\n",
              "       10,  5, 11,  2,  0,  0,  0,  3,  0,  0,  5,  5,  6, 16, 25,  0,  0,\n",
              "        9,  7,  0,  4,  0,  9,  0,  0,  0,  3,  6,  3,  3,  4,  6, 19,  9,\n",
              "        7,  7,  0,  0,  0,  5, 10, 13, 17, 12,  0,  0,  5, 12, 20, 16,  0,\n",
              "        0, 10,  1,  3,  6,  8,  0,  9,  4,  3, 10, 10,  6,  0,  4,  0,  7,\n",
              "        6,  2, 12,  0,  0, 57,  0,  1,  5,  9,  0,  7,  0, 10, 17,  8, 16,\n",
              "        9,  5,  8, 13,  2, 15,  0,  0, 12,  0,  9, 15,  0, 11,  2,  0,  3,\n",
              "        0,  2,  0, 14, 14, 11,  0, 16,  5,  8,  0, 10,  8,  0,  5,  0, 11,\n",
              "       15,  3,  0, 10, 16,  0, 15,  6,  0,  8,  0, 10,  3,  6, 24,  0,  3,\n",
              "        0, 13, 21,  5,  0,  4,  0, 10,  0, 10,  7,  0,  1,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPgSKEX9k_Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = []\n",
        "for doc in tokenizer.pipe(x_train, batch_size=500):\n",
        "  doc_tokens = []\n",
        "  for token in doc:\n",
        "      doc_tokens.append(token.text.lower())\n",
        "  tokens.append(doc_tokens)\n",
        "x_train = tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6T9mkeYlLLz",
        "colab_type": "code",
        "outputId": "24c3adb8-59e3-4ee9-d7f1-1e537dead324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "x_train = [\" \".join(row) for row in x_train]\n",
        "# X_test = [\" \".join(row) for row in X_test]\n",
        "\n",
        "x_train = vect.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_train.todense(), columns=vect.get_feature_names())\n",
        "x_train.head()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>00 application</th>\n",
              "      <th>000</th>\n",
              "      <th>000 adults</th>\n",
              "      <th>000 depending</th>\n",
              "      <th>000 different</th>\n",
              "      <th>000 employees</th>\n",
              "      <th>000 nerds</th>\n",
              "      <th>000 people</th>\n",
              "      <th>000 person</th>\n",
              "      <th>000 preventable</th>\n",
              "      <th>01</th>\n",
              "      <th>01 unnecessary</th>\n",
              "      <th>01 variable</th>\n",
              "      <th>01 x27</th>\n",
              "      <th>01 x2f</th>\n",
              "      <th>0134555</th>\n",
              "      <th>0134555 rel</th>\n",
              "      <th>02</th>\n",
              "      <th>02 x2f</th>\n",
              "      <th>021</th>\n",
              "      <th>021 025h</th>\n",
              "      <th>025h</th>\n",
              "      <th>025h 170</th>\n",
              "      <th>02684527</th>\n",
              "      <th>02684527 2020</th>\n",
              "      <th>03</th>\n",
              "      <th>03 31</th>\n",
              "      <th>039162761</th>\n",
              "      <th>039162761 ebola</th>\n",
              "      <th>0391627617</th>\n",
              "      <th>0391627617 ci23x11</th>\n",
              "      <th>04</th>\n",
              "      <th>04 12</th>\n",
              "      <th>04 x2f</th>\n",
              "      <th>04f5osxk4vw</th>\n",
              "      <th>04f5osxk4vw rel</th>\n",
              "      <th>05</th>\n",
              "      <th>05 19</th>\n",
              "      <th>05 27</th>\n",
              "      <th>...</th>\n",
              "      <th>zettelkasten</th>\n",
              "      <th>zettelkasten href</th>\n",
              "      <th>zettelkasten imho</th>\n",
              "      <th>zettelkasten x2f</th>\n",
              "      <th>zhfpbw</th>\n",
              "      <th>zhfpbw nuwk</th>\n",
              "      <th>zim</th>\n",
              "      <th>zim past</th>\n",
              "      <th>zionists</th>\n",
              "      <th>zionists jews</th>\n",
              "      <th>zip</th>\n",
              "      <th>zip links</th>\n",
              "      <th>zone</th>\n",
              "      <th>zone status</th>\n",
              "      <th>zones</th>\n",
              "      <th>zones place</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zoom built</th>\n",
              "      <th>zoom disabled</th>\n",
              "      <th>zoom look</th>\n",
              "      <th>zoom meetings</th>\n",
              "      <th>zoom x2f</th>\n",
              "      <th>zoox</th>\n",
              "      <th>zoox billion</th>\n",
              "      <th>zoox dumped</th>\n",
              "      <th>zoox x27</th>\n",
              "      <th>zotero</th>\n",
              "      <th>zotero physicists</th>\n",
              "      <th>zotero switched</th>\n",
              "      <th>zotero track</th>\n",
              "      <th>zuckerberg</th>\n",
              "      <th>zuckerberg facebook</th>\n",
              "      <th>ωb</th>\n",
              "      <th>ωb 051</th>\n",
              "      <th>карта</th>\n",
              "      <th>карта спутниковой</th>\n",
              "      <th>обстановки</th>\n",
              "      <th>обстановки rooms</th>\n",
              "      <th>спутниковой</th>\n",
              "      <th>спутниковой обстановки</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 36257 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    00  00 application  ...  спутниковой  спутниковой обстановки\n",
              "0  0.0             0.0  ...          0.0                     0.0\n",
              "1  0.0             0.0  ...          0.0                     0.0\n",
              "2  0.0             0.0  ...          0.0                     0.0\n",
              "3  0.0             0.0  ...          0.0                     0.0\n",
              "4  0.0             0.0  ...          0.0                     0.0\n",
              "\n",
              "[5 rows x 36257 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geTuIc3rmU5-",
        "colab_type": "code",
        "outputId": "b9c0661d-1aa4-4722-c52d-b1c4eb7a45a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(penalty='l2', max_iter=100)\n",
        "\n",
        "lr.fit(x_train,y_train)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RftOXLpMmgcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = vect.transform(['really bad comment','I love this'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzJBAaA672LL",
        "colab_type": "code",
        "outputId": "2cfba687-cbc1-4860-ff05-2915a92eac59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lr.predict(test)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0F5c3NZc2BU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = subset['Comment'].values\n",
        "y_train = subset['sent_score'].apply(toint).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muk-L3nj75EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # strings to token integer counts\n",
        "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
        "    ('classifier', LogisticRegression(random_state=101)),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
        "])\n",
        "\n",
        "pipeline.fit(x_train,y_train)\n",
        "predictions = pipeline.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWJgTy1xcPk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subset['pred'] = predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5SOADyzc54P",
        "colab_type": "code",
        "outputId": "2f0ff3f8-a610-41f4-fd69-e7cc2f2c56e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "subset[subset['pred'] > 6]['Comment'].values"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['It&#x27;s not that I want to visit the store it&#x27;s that I can&#x27;t trust the store people to properly pick out fresh produce or good meat.<p>When the stock person just takes a bucket of apples and dumps them in without care so most of them are bruised, why would I want that same person selecting which apples to send me?  Often times to find 3 apples I have to examine 10+.  Most of the produce selection is this way.<p>Meat selection is not much different.  Selecting chicken without careful examination you&#x27;ll get broken legs or wings.<p>Now you have to consider the automatic substitution of equivalent items when something is out of stock.  My dibetic friend was telling yesterday that they substituted regular mt dew for his order of mt dew zero sugar.<p>Until the store starts employing people who care about product selection as much as I do, then I&#x27;ll continue to make time to go to the store and pick it myself.',\n",
              "       '<i>They won&#x27;t spit it out directly of course but people talk and what they say is that I need to be stellar or young to be hired. Companies won&#x27;t invest in me the slightest bit, so the moment I miss a question in the long interview process I&#x27;m out of the door without second thought.</i><p>You&#x27;re saying is that you&#x27;re not a good fit <i>technically</i> and would need some investment in terms of either time or training, and companies are choosing not to go with that option. Why is that ageism?',\n",
              "       'I thought this was a pretty fair article that rightly addressed some of the major structural issues with remote work as it currently is set up. I&#x27;ve worked remotely for ~5 years and absolutely love it but think there need to be much larger corporate culture changes needed, along with technologies, to make it a sustainable trend for 30%+ of a modern workforce.<p>I do also wonder if there might be some wrong lessons taken from this time. This isn&#x27;t just &quot;working from home&quot; it&#x27;s &quot;working from home during a global health crisis.&quot; Schools are closed, people we know are dying, lockdowns, there&#x27;s fear and uncertainty to a degree I hadn&#x27;t before seen in my adult life ... so yeah, it&#x27;s going to have an impact on our workflows.<p>Like any change, it&#x27;s always going to be hardest for large orgs who have to change vs those who have it built-in from the beginning.',\n",
              "       'I agree with the premise of this article. That armchair quarterbacking is rarely helpful and usually oversimplified.<p>But I dislike the common implication in the headline, which is that intelligence somehow always equates to wealth.<p>First of all, it&#x27;s not always easy to judge how wealthy another person is, no matter their profession or outward appearance.<p>Secondly, I think we all need to disuade ourselves of the notion that everyone in the world wants to become wealthy. Plenty of extremely intelligent people are perfectly happy where they are and are perhaps smarter for not chasing wealth like all those genius entrepreneurs. Just look at the lives of those we idolize so much for their success. Do you really want that life? A lot don&#x27;t. And there is a good chance people more intelligent than those &quot;successful&quot; people don&#x27;t want that life either.<p>Additionally, I know quite a few wealthy people that are rather dim. But they were hard workers and an opportunity presented itself because they were at the right place at the right time and they took advantage of that opportunity and worked it dry.',\n",
              "       'No they can&#x27;t. They don&#x27;t have the experience that I have. They can&#x27;t possibly have it because they don&#x27;t already work for the company I work for.<p>I think some people in &quot;the valley&quot; are starting to brick it knowing that they&#x27;ll be competing with people with far lower salary demands from around the world. But really the only think you&#x27;ll have to worry about if your job goes remote is having to move somewhere more affordable.',\n",
              "       'I have a strong opinion on this, but it starts at a lower level...<p>A good developer is a developer that makes themselves obsolete.<p>By making themselves obsolete the developer has made themselves invaluable.<p>How can both of those things be true?<p>This is basically because you find that great developers write code and patterns that make it easy for anyone to jump in an understand. Even if complicated. I strive to do this as much as possible. The more I do this the faster development becomes for myself and my team. However the more I do this, the more my team leads realize that without me, things won&#x27;t progress in the right direction. But theoretically they can fire me any time and find a cheaper worker to do my job.<p>The point is, I see lots of managers treat engineers as commodities. However if that is their outlook, then they will always look to outsource because they see no value in you. Good managers realize that things are smooth and maintainable because of the people who put effort into doing that, not that because things are smooth and maintainable everyone is expandable. Every time I see the latter being the mindset there&#x27;s always a HUGE slowdown because the wrong things start being built.',\n",
              "       'My experience is anecdotal and second hand. I&#x27;ve seen it twice now.<p>It began with a friend who was in the job market as a 50+. More on the hardware side. This guy has some cool experience. He gets lots of interviews, they go well, but no offers.<p>As his frustration grows, he grows desperate to try something different. He dyes some color back into his hair, gets some tinted glasses, and lets his daughter take him shopping for some more hip interview clothes.<p>A month later and he&#x27;s in bidding wars for who to hire him. He said the difference was night and day. He was now pointing out his age in interviews &quot;are you sure I&#x27;m not too old?&quot; and the interviewers were like &quot;no way man.&quot;<p>I wondered how one off this was. A year or so later, knew another guy who was having this same struggle. We shared the story with him. He raised his eyebrows, hesitated for a week or two, the colored his hair, got his niece to take him shopping. And pretty much same thing.<p>Obviously, this is a small sample set. But the lesson I took from this (and haven&#x27;t had a chance to prove for myself yet) is that it&#x27;s not your age that will limit you, but your apparent age. If you are old, but look like a younger&#x2F;fresher version of yourself, you do well. If you appear &quot;old&quot;, you struggle.<p>Best of luck.',\n",
              "       'This is a good thing!<p>Google, Facebook, Twitter and Reddit are pretty heavy handed when it comes to &quot;censoring&quot; content. This is annoying because somehow &quot;rightwing &#x2F; wrongthink&quot; content is censored while &quot;leftwing&quot; content is fine as it is.<p>If they are not platforms and are publishers - they need to be treated as such.<p>I feel this needs to be extended all the way down - Why stop at social media? Let&#x27;s go ahead and hit where this counts - Domain Registrars, DNS Hosts, Payment Processors and Web hosts.<p>The sad part is - if they do, the internet as we know won&#x27;t exist anymore. But in all honesty, we are heading to it anyways - with all the internet companies consolidating - I don&#x27;t think I&#x27;ll loose a lot of sleep if Twitter if fined - They had it coming for a while anyways.<p>EDIT: If this comment does not follow the rules, I&#x27;m happy to delete it as it&#x27;s inherently political imo. Also I&#x27;m happy to provide examples of the above claims as well.',\n",
              "       'It&#x27;s a strange decision by Twitter here, but they are in a strange position.<p>Donny, love him or hate him, does say a fair few things that are ... questionable. Jack has talked about this a bit, and their conclusion thus far has been that anything he says, by virtue of the office, is newsworthy enough. Policies for thee, but not for he. It&#x27;s been a battle with users, but everyone seems to just grumble along.<p>That policy has worked up until today.<p>A lot of work went into this decision. They A&#x2F;B tested the color of the note, likely the font, the positioning, the exact words, the fact check itself, etc. This thing went through meeting after meeting and was run past some good legal counsel. Twitter isn&#x27;t the sharpest tool in the shed, but it&#x27;s also not a rusty shovel. They red-teamed this a fair bit, I&#x27;d imagine. They must have known that Donny would not view it favorably and would do exactly what he is doing currently.<p>All the same they went ahead and decided to make the move at the end of May, ~6 months before the &#x27;fit hits the shan&#x27;.<p>Why?<p>Their stock is, well, fairly ok. Jack seems to be doing alright. Monthly users are flat-ish since 2015, but compared to FB, it&#x27;s a bit of a wash.',\n",
              "       'Its evident to me that our strategy to combat misinformation is not going great at the moment. I&#x27;ve been on Reddit for over 13 years and the site has gone through many changes.<p>What if we changed our thinking from removing&#x2F;flagging bad content to fostering rich discourse?<p>I&#x27;ll use r&#x2F;politics for example, I currently do not think there is productive or rich discourse being had there. If you have had a different experience please let me know.<p>I think for the political arena it would do us good to try to emulate the US House of Representatives where representatives are given equal time to address the floor. In this way you will be exposed to other perspectives. The ways we can achieve this are similar to the approach NYT has taken to comments. You can still sort comments by most recommended, but there are also &quot;Featured Comments&quot;. Featured Comments are chosen by a team at NYT, presumably from ideologically diverse perspectives, and they choose comments that are insightful and rich in information without toxicity. Does anyone else think that would be a good idea?<p>I think its important because I truly believe Americans are far more alike then different and just about everyone feels like they are under attack or have been violated. Its time to heal and listen and understand that we are in it together and the people that we really should be castigating are the people filled with prejudice to the point where they have shut themselves off from hearing other perspectives. I believe there is a vast middle in the USA, but its currently getting drowned out and it should have a louder voice.',\n",
              "       'I got a job after 18 months of looking<p>I&#x27;m doing some bullshit specialty software i hate but that I happen to know pretty well<p>The lesson there, to me, is think about specializing. Why?<p>Because...why not?<p>Outside of that<p>My advice to older folks looking for IT work is<p>It is not going to happen -- think about stocking shelves or anything you can<p>Once you come to terms with the situation, then you can get real about what is really required to get a job<p>A miracle and tons of hard work of the type you don&#x27;t want to do<p>I&#x27;ve seen lots of good advice on here that I think is pretty good<p>Like<p>Look younger \\nAct younger\\nDress younger \\nBe younger<p>Reach out on LinkedIn and other places -- it won&#x27;t help but I think it is important to check the boxes -- it&#x27;s a pretty good way to quickly get to rock bottom shame or shamelessness -- completely remove your ego from the equation<p>Even busted my ass for an AWS cert -- worthless<p>I&#x27;ve started losing weight and people are noticing<p>I figure each 10 lbs you lose takes off a year or two of age<p>What would I do if I got shitcanned tomorrow?<p>I would probably become an &#x27;out&#x27; specialist in this particular software I know<p>I _hate_ this gd software<p>But a job is a job<p>There have also been sites that claimed to specialize in helping to hire older workers<p>I figure it was just a scam but I would also check it out<p>I did occasionally get play from startups that I was actually interested in -- by writing authentic-ish notes of interest<p>But yeah nobody but the 1%ers are getting jobs in this market<p>And that is some weird mix of the geniuses, connected, etc.<p>Nobody else getting hired -- I don&#x27;t care if you are 25 yo or 75 yo -- not happening',\n",
              "       'It&#x27;s so depressing how low the bar is, for regulatory frameworks.<p>Maybe it&#x27;s totally co-opted by industry incumbents, and serves as a moat against competition. Maybe it&#x27;s about electoral politics. IE, the problem is stifling of specific voices supportive of Trump... or the opposite. We can&#x27;t even hope that it will be designed logically to serve a principle or goal.<p>I&#x27;m not saying regulation (or better legislation) isn&#x27;t necessary. Twitter set out to be the &quot;public square&quot; and succeeded. It&#x27;s a privately controlled public square. This <i>is</i> a problem for democracy. It&#x27;s no joke either. Twitter &amp; Facebook can and have been the launch point for elections, revolutions, coups and such. &quot;It&#x27;s their site, they can do what they want&quot; is not, IMO, reasonable.<p>On a related point &quot;platform monopolies,&quot; are a well established problem, and we&#x27;ve even had some court ruling to that effect. Courts are narrow though. Legislation is the only way to establish principles.<p>This should be a &quot;step back and think big picture&quot; situation. Establish principles that guarantee freedoms, media diversity  or whatever the goals are. Instead, the debate will be dictated by electoral considerations... on both the pro and con side. I doubt they&#x27;re even thinking past the immediate election.',\n",
              "       'As several people have pointed out, both the &quot;Personal&quot; and &quot;Catalyst&quot; licenses are intended for personal use only, making this quite an expensive product should I want to use it for work as well. But in the full license text, there seems to be an even more problematic phrasing:<p>&gt; The use of OBSIDIAN for the exercise of your own trade or profession (...) does not qualify as personal use.<p>I would interpret this to mean that I as a developer can not use this to exercise the &quot;trade&quot; of software development. That would in turn mean that I can not use this to make notes of stuff I learn on my own time, if it is related to software development.<p>I would imagine most people not caring about this kind of license limitation, but it would be interesting if it was intended this way, or if this is just me being bad at licenses.',\n",
              "       'Most online courses I’ve taken (university or MOOC) were not improvements over simply reading the textbook&#x2F;source and doing the published exercises. I have taken a handful of online courses from “real universities” and they have all been atrociously bad. Some MOOCs I have taken have been much nicer (Ng’s Machine Learning stands out). Courses on platforms like Udemy seem to almost uniformly consist of regurgitated documentation. Maybe that’s helpful for someone (or they wouldn’t be so popular (?)), but I find video to be a slower and less dense form of info transmission.<p>The main thing I find annoying is that most of the courses I’ve tried don’t leverage the interactivity available to them. The exercises don&#x27;t seem to have the right level of challenge to enable flow. In college, I took a linear algebra course with lectures on one day and group work on the other. On the second day, we’d have a difficult application problem of whatever we learned earlier in the week. We’d break into groups (small enough that you couldn’t hide) and work through the hard problem together. Each time, I’d leave the class feeling like I had truly gained a deep understanding of the subject matter.<p>In contrast, most online courses (if they have exercises at all) seem to be of the form: show pattern, change obvious detail, ask for obvious implementation. I haven’t found a lot of exercises that actually require a stretched understanding of the material to get through. Maybe this is optimized to mitigate huge drop off rates in MOOCs - easy problems keep people around longer. But, that doesn&#x27;t really create a valuable learning experience.',\n",
              "       'This is a well written article and has really good courses and topics in it.<p>Cynically though, I wonder how many people of those who liked it and file it under &quot;I should get to this at some point&quot; actually use it at all. In AI it&#x27;s been a cliche now how many introductory blog posts and into YouTube videos and &quot;how do I start&quot; Reddit and Quora questions there are. The resources and buzz is very high for the first steps. I guess it&#x27;s similar to &quot;how do I make a game&quot; or &quot;hó can I learn to hack&quot; are popular.<p>As I said this one is very solid and has very actionable pointers to great courses. But I find that only a small minority of people are so obsessed that they can pull this huge project on their own. It&#x27;s a multi-year undertaking and even though the resources are freely available, a normal homo sapiens just doesn&#x27;t work like this. For all but the wild outliers (who would find the info anyway, with no Obstacle able to stop them), the context of a university program is really necessary. It gives you time, structure and social motivation, discussing with people in the same boat, helping them out, getting help from them, in person, being forced to chew through the boring bits, not getting satisfied with your own self assessment etc.<p>Otherwise I think this also just gets thrown onto the pile of bookmarks that we all build, to make ourselves feel good about a future day where we somehow magically motivated and sharp to tackle and learn all those things we bookmarked.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0TRfPx-dDDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uDJAHBSfHt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_FILEPATH = os.path.join(\"latest_model.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOwdCqq3gASH",
        "colab_type": "code",
        "outputId": "a5437f43-13be-4f80-d51e-be0e5cbcfe96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"SAVING THE MODEL...\")\n",
        "with open(MODEL_FILEPATH, \"wb\") as model_file:\n",
        "  pickle.dump({\"model\": pipeline}, model_file)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAVING THE MODEL...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5trVWGUIgfVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model():\n",
        "    print(\"LOADING THE MODEL...\")\n",
        "    with open(MODEL_FILEPATH, \"rb\") as model_file:\n",
        "        saved_model = pickle.load(model_file)\n",
        "    return saved_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6fUWcrwhUXA",
        "colab_type": "code",
        "outputId": "d2b0d4ce-8b4c-4a00-fca5-e33942581e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "package = load_model()\n",
        "model = package['model']\n",
        "pred = model.predict(x_train)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOADING THE MODEL...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvmzTiN7heHy",
        "colab_type": "code",
        "outputId": "6faeaca5-a47d-4a72-c098-d326e414dcc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "source": [
        "pred"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 9, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 5,\n",
              "       0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 5, 0, 0, 0, 7, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 4, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 7, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 9, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 6, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 5, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 5, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    }
  ]
}